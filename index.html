<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shuaiyi Huang</title>

    <meta name="author" content="Shuaiyi Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shuaiyi Huang
                </p>
                <p>I'm a PhD student in the department of Computer Science at <a href="https://umd.edu/">University of Maryland, College Park</a>, advised by Prof. <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>.
                </p>
                <p>
                  I obtained my M.S. degree in Computer Science from
                  <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a> in 2020.
                  I received my B.Eng. degree in Software Engineering from
                  <a href="https://en.tongji.edu.cn/p/#/">Tongji University</a> in 2017.
                <p>

                <p>
                  My research interests lie in Computer Vision and Autonomous Agents, focusing on solving problems with
limited or noisy supervision. I aim to enable AI agents to understand the visual world better and to develop
multi-modal AI systems that integrate vision, language, and action understanding.
                </p>

                <p style="text-align:center">
                  <a href="mailto:huangshy@umd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_ShuaiyiHuang_UMD.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=j-P28bQAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/shuaiyi-huang-254a6b1b1/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ShuaiyiHuang">Github</a>


                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/hsy/shuaiyi.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/hsy/shuaiyi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            

            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/trend.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://mlevy2525.github.io/">Mara Levy</a>,
                  <a href="https://learn2phoenix.github.io/ek">Anubhav Gupta</a>,
                  <a href="https://danielekpo.com/">Daniel Ekpo</a>,
                  <a href="https://ruijiezheng.com/">Ruijie Zheng</a>,
                  <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                  <br>
                  <em>ICRA</em>, 2025
                  <br>
              
                  <p></p>
                  <!-- <p>We introduce a tri-teaching framework for learning from noisy preferences in reinforcement learning, utilizing demonstrations to enhance policy robustness.</p> -->
                   <p>Preference feedback collected by human or VLM
                    annotators is often noisy, presenting a significant challenge for
                    preference-based reinforcement learning. To address this challenge, we propose
                    TREND, a novel framework that integrates few-shot expert
                    demonstrations with a tri-teaching strategy for effective noise
                    mitigation. </p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/tracevla2.jpg' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</papertitle>
                  <br>
                  <a href="https://ruijiezheng.com/">Ruijie Zheng*</a>, <a href="https://cheryyunl.github.io/">Yongyuan Liang*</a>,
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en">Jianfeng Gao</a>,
                  <a href="https://scholar.google.com/citations?user=PbEw81gAAAAJ&hl=en">Hal Daum&#233; III</a>,
                  <a href="https://scholar.google.com/citations?user=xEWgxBsAAAAJ&hl=en">Andrey Kolobov</a>,
                  <a href="https://furong-huang.com/">Furong Huang</a>,
                  <a href="https://jwyang.github.io/">Jianwei Yang</a>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://tracevla.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2412.10345">arxiv</a> / 
                  <a href="https://github.com/umd-huang-lab/tracevla">code</a>
              
                  <p></p>
                  <p> In this work, we introduce visual
                    trace prompting, a simple yet effective approach to facilitate Vision Language Action Models' spatial-
                    temporal awareness for action prediction.
                    visually.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/autohallu.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models</papertitle>
                  <br>
                  <a href="https://wuxiyang1996.github.io/">Xiyang Wu*</a>, <a href="#">Tianrui Guan*</a>,
                  <a href="https://scholar.google.com/citations?user=K40nbiQAAAAJ&hl=en">Dianqi Li</a>, <strong>Shuaiyi Huang</strong>, 
                  <a href="https://scholar.google.com/citations?user=EcHTiyIAAAAJ&hl=en">Xiaoyu Liu</a>, <a href="https://xijun-cs.github.io/">Xijun Wang</a>, 
                  <a href="https://ricky-xian.github.io/">Ruiqi Xian</a>, <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
                  <a href="https://furong-huang.com/">Furong Huang</a>, <a href="https://users.umiacs.umd.edu/~jbg/">Jordan Lee Boyd-Graber</a>,
                  <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>, <a href="https://scholar.google.com/citations?user=X08l_4IAAAAJ&hl=en">Dinesh Manocha</a>
                  <br>
                  <em>EMNLP Findings</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10900">arxiv</a> / 
                  <a href="https://github.com/wuxiyang1996/AutoHallusion">code</a>
              
                  <p></p>
                  <p>A benchmark framework that automatically generates hallucination cases in Vision-Language models to evaluate their robustness and accuracy.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/ardup.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>ARDuP: Active Region Video Diffusion for Universal Policies</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://mlevy2525.github.io/">Mara Levy</a>,
                  <a href="https://zhenyujiang.me/">Zhenyu Jiang</a>,
                  <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>,
                  <a href="https://yukezhu.me/">Yuke Zhu</a>,
                  <a href="https://jimfan.me/">Linxi Fan</a>,
                  <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
                  <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                  <br>
                  
                  <em>IROS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2406.13301">arxiv</a> / 
                  <a href="#">code</a>
              
                  <p></p>
                  <p>We propose a novel method for universal policy learning via active region video diffusion models, focusing on task-critical regions in videos.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/pointvis.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>What is Point Supervision Worth in Video Instance Segmentation?</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
                  <a href="https://chrisding.github.io/">Zhiding Yu</a>,
                  <a href="https://voidrank.github.io/">Shiyi Lan</a>,
                  <a href="https://www.linkedin.com/in/subhashree-radhakrishnan-b0b0048b/">Subhashree Radhakrishnan</a>,
                  <a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a>,
                  <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
                  <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
                  <br>
                  <em>CVPR Workshop on Learning With Limited Labelled Data for Image and Video Understanding</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2404.01990">arxiv</a> 
              
                  <p></p>
                  <p>This work explores the impact of point-level supervision in the context of video instance segmentation, offering insights into its effectiveness.</p>
              </td>
            </tr>
            
            

            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/uvis.jpg' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>UVIS: Unsupervised Video Instance Segmentation</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://www.cs.umd.edu/~sakshams/">Saksham Suri</a>,
                  <a href="https://kampta.github.io/">Kamal Gupta</a>,
                  <a href="https://rssaketh.github.io/">Sai Saketh Rambhatla</a>,
                  <a href="https://sites.google.com/site/sernam">Ser-nam Lim</a>,
                  <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                  <br>
                  <em>CVPR Workshop on Learning With Limited Labelled Data for Image and Video Understanding</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.06908">arxiv</a> 
              
                  <p></p>
                  <p>We propose an unsupervised approach for video instance segmentation, leveraging self-supervised learning methods to improve object instance tracking and segmentation across video frames.</p>
              </td>
            </tr>

            


            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/D-NeRV.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Towards Scalable Neural Representation for Diverse Videos</papertitle>
                </a>
                <br>
                <a href="https://boheumd.github.io/">Bo He</a>,
                <a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&hl=en">Xitong Yang</a>,
                <a href="https://hywang66.github.io/">Hanyu Wang</a>,
                <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
                <a href="https://haochen-rye.github.io/">Hao Chen</a>,
                <strong>Shuaiyi Huang</strong>,
                <br>
                <a href="https://scholar.google.com/citations?user=TFpdak8AAAAJ&hl=en">Yixuan Ren</a>,
                <a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser-Nam Lim</a>,
                <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                <br>
                <em>CVPR</em>, 2023  
                <br>
                <a href="https://boheumd.github.io/D-NeRV">project page</a> / 
                <a href="https://arxiv.org/abs/2303.14124">arxiv</a> / 
                <a href="https://github.com/boheumd/D-NeRV">code</a>  
  
                <p></p>
                <p>We propose D-NeRV, a novel implicit neural representation based framework designed to encode large-scale and diverse videos. It achieves state-of-the-art performances on video compression.</p>
              </td>
            </tr>




            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/SCSA.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Learning Semantic Correspondence with Sparse Annotations</papertitle>
                </a>
                <br>
                <strong>Shuaiyi Huang</strong>,
                <a href="https://www.loyo.me/">Luyu Yang</a>,
                <a href="https://boheumd.github.io/">Bo He</a>,
                <a href="https://www.zhangsongyang.com/">Songyang Zhang</a>,
                <a href="https://xmhe.bitbucket.io/">Xuming He</a>,
                <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                <br>
                <em>ECCV</em>, 2022
                <br>
                <a href="https://shuaiyihuang.github.io/publications/SCorrSAN/">project page</a> / 
                <a href="https://arxiv.org/pdf/2208.06974.pdf">arxiv</a> / 
                <a href="https://github.com/shuaiyihuang/SCorrSAN">code</a>  
  
                <p></p>
                <p>We address the challenge of label sparsity in semantic correspondence by enriching supervision signals from sparse keypoint annotations. We first propose a teacher-student learning paradigm for generating dense pseudo-labels and then develop two novel strategies for denoising pseudo-labels. </p>
              </td>
  
            </tr> 
    


            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/camnet.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Confidence-aware Adversarial Learning for Self-supervised Semantic Matching</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://scholar.google.com/citations?user=mCPax7QAAAAJ&hl=en">Qiuyue Wang</a>,
                  <a href="https://xmhe.bitbucket.io/">Xuming He</a>
                  <br>
                  <em>PRCV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2008.10902">arxiv</a> / 
                  <a href="https://github.com/ShuaiyiHuang/CAMNet">code</a>
              
                  <p></p>
                  <p>This paper explores a confidence-aware adversarial learning framework to enhance self-supervised semantic matching with improved robustness and accuracy.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/dehaze_tip2020.jpg' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Dehazing Evaluation: Real-World Benchmark Datasets, Criteria, and Baselines</papertitle>
                  <br>
                  <a href="https://xiaofeng94.github.io/">Shiyu Zhao</a>, 
                  <a href="https://cslinzhang.github.io/home/">Lin Zhang</a>,
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://scholar.google.com/citations?user=A0N_mS0AAAAJ&hl=en">Ying Shen</a>,
                  <a href="#">Shengjie Zhao</a>
                  <br>
                  <em>TIP</em>, 2020
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9099036">paper</a> / 
                  <a href="https://github.com/xiaofeng94/BeDDE-for-defogging">code</a>
              
                  <p></p>
                  <p>This work presents real-world benchmark datasets, evaluation criteria, and baseline approaches for assessing dehazing methods in image processing.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/dccnet.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Dynamic Context Correspondence Network for Semantic Alignment</papertitle>
                  <br>
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://scholar.google.com/citations?user=mCPax7QAAAAJ&hl=en">Qiuyue Wang</a>,
                  <a href="http://www.zhangsongyang.com/">Songyang Zhang</a>,
                  <a href="https://yanshipeng.com/">Shipeng Yan</a>,
                  <a href="https://xmhe.bitbucket.io/">Xuming He</a>
                  <br>
                  <em>ICCV</em>, 2019
                  <br>
                  <a href="https://arxiv.org/abs/1909.03444">arxiv</a> / 
                  <a href="https://github.com/ShuaiyiHuang/DCCNet">code</a>
              
                  <p></p>
                  <p>We introduce a Dynamic Context Correspondence Network (DCCN) to improve semantic alignment by leveraging dynamic feature contexts across images.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/evaldefog_2019icme.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Evaluation of Defogging: A Real-world Benchmark Dataset, A New Criterion and Baselines</papertitle>
                  <br>
                  <a href="https://xiaofeng94.github.io/">Shiyu Zhao</a>, 
                  <a href="https://cslinzhang.github.io/home/">Lin Zhang</a>,
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://scholar.google.com/citations?user=A0N_mS0AAAAJ&hl=en">Ying Shen</a>,
                  <a href="#">Shengjie Zhao</a>,
                  <a href="#">Yukai Yang</a>
                  <br>
                  <em>ICME</em>, 2019
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/8784729">paper</a> / 
                  <a href="https://github.com/xiaofeng94/BeDDE-for-defogging">code</a>
              
                  <p></p>
                  <p>This paper provides a real-world benchmark dataset for defogging, a new evaluation criterion, and baseline approaches to assess defogging techniques.</p>
              </td>
            </tr>
            
            <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
              <td style="padding:20px;width:30%;vertical-align:middle">
                <img src='images/projects/structurevqa2017.png' width="240">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Structured Attentions for Visual Question Answering</papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=m-om5O8AAAAJ&hl=en">Chen Zhu</a>, 
                  <a href="https://scholar.google.com/citations?user=-T9FigIAAAAJ&hl=en">Yanpeng Zhao</a>,
                  <strong>Shuaiyi Huang</strong>,
                  <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/tukw/">Kewei Tu</a>,
                  <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
                  <br>
                  <em>ICCV</em>, 2017
                  <br>
                  <a href="https://arxiv.org/abs/1708.02071">paper</a> / 
                  <a href="https://github.com/shtechair/vqa-sva">code</a>
              
                  <p></p>
                  <p>This work proposes structured attention mechanisms for visual question answering, enabling more precise reasoning over complex visual scenes.</p>
              </td>
            </tr>



          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Services</h2>
                <ul>
                  <li>Reviewers: CVPR, ECCV, ICCV, IJCV, TIP, WACV, ACCV, ICRA
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Experiences</h2>
                <ul>
                  <li>Research Intern at NVIDIA, 2023. I was fortunate to work with  <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,  <a href="https://jimfan.me/">Jim Fan</a>, and <a href="https://yukezhu.me/">Yuke Zhu</a>.
                  <li>Research Intern at NVIDIA, 2022. I was fortunate to work with <a href="https://chrisding.github.io/">Zhiding Yu</a>, <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>, <a href="https://voidrank.github.io/">Shiyi Lan</a>, <a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a>,  <a href="https://www.linkedin.com/in/subhashree-radhakrishnan-b0b0048b/">Subhashree Radhakrishnan</a>, and  <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>.
                </ul>
              </td>
            </tr>
          </tbody></table>
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">Thank <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing the source code of his personal page.</p>
                 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
